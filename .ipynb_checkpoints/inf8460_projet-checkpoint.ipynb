{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement du corpus Wikipédia  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-meta-current.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ouvrons l'archive avec l'option k afin de garder le fichier d'origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bzip2 -dk /store/dyfar/enwiki-latest-pages-meta-current.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant nettoyer le fichier xml. Pour ce faire nous utilisons wikiextractor.\n",
    "\n",
    "Beaucoup d'options sont disponibles et vous pouvez les changer si vous le voulez. Le bloc suivant installe wikiextractor, créé ensuite un dossier dans lequel le script va ajouter les fichiers une fois nettoyer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wikiextractor\n",
    "! mkdir /store/dyfar/wikipedia_dump\n",
    "! python3 -m wikiextractor.WikiExtractor /store/dyfar/enwiki-latest-pages-meta-current.xml -o /store/dyfar/wikipedia_dump --json --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA  AI\tAQ  AY\tBG  BO\tBW  CE\tCM  CU\tDC  DK\tDS  EA\tEI  EQ\tEY  FG\n",
      "AB  AJ\tAR  AZ\tBH  BP\tBX  CF\tCN  CV\tDD  DL\tDT  EB\tEJ  ER\tEZ  FH\n",
      "AC  AK\tAS  BA\tBI  BQ\tBY  CG\tCO  CW\tDE  DM\tDU  EC\tEK  ES\tFA  FI\n",
      "AD  AL\tAT  BB\tBJ  BR\tBZ  CH\tCP  CX\tDF  DN\tDV  ED\tEL  ET\tFB  FJ\n",
      "AE  AM\tAU  BC\tBK  BS\tCA  CI\tCQ  CY\tDG  DO\tDW  EE\tEM  EU\tFC  FK\n",
      "AF  AN\tAV  BD\tBL  BT\tCB  CJ\tCR  CZ\tDH  DP\tDX  EF\tEN  EV\tFD\n",
      "AG  AO\tAW  BE\tBM  BU\tCC  CK\tCS  DA\tDI  DQ\tDY  EG\tEO  EW\tFE\n",
      "AH  AP\tAX  BF\tBN  BV\tCD  CL\tCT  DB\tDJ  DR\tDZ  EH\tEP  EX\tFF\n",
      "wiki_00  wiki_12  wiki_24  wiki_36  wiki_48  wiki_60  wiki_72  wiki_84\twiki_96\n",
      "wiki_01  wiki_13  wiki_25  wiki_37  wiki_49  wiki_61  wiki_73  wiki_85\twiki_97\n",
      "wiki_02  wiki_14  wiki_26  wiki_38  wiki_50  wiki_62  wiki_74  wiki_86\twiki_98\n",
      "wiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87\twiki_99\n",
      "wiki_04  wiki_16  wiki_28  wiki_40  wiki_52  wiki_64  wiki_76  wiki_88\n",
      "wiki_05  wiki_17  wiki_29  wiki_41  wiki_53  wiki_65  wiki_77  wiki_89\n",
      "wiki_06  wiki_18  wiki_30  wiki_42  wiki_54  wiki_66  wiki_78  wiki_90\n",
      "wiki_07  wiki_19  wiki_31  wiki_43  wiki_55  wiki_67  wiki_79  wiki_91\n",
      "wiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92\n",
      "wiki_09  wiki_21  wiki_33  wiki_45  wiki_57  wiki_69  wiki_81  wiki_93\n",
      "wiki_10  wiki_22  wiki_34  wiki_46  wiki_58  wiki_70  wiki_82  wiki_94\n",
      "wiki_11  wiki_23  wiki_35  wiki_47  wiki_59  wiki_71  wiki_83  wiki_95\n",
      "{\"id\": \"12\", \"url\": \"https://en.wikipedia.org/wiki?curid=12\", \"title\": \"Anarchism\", \"text\": \"Anarchism\\n\\nAnarchism is a political philosophy and movement that rejects all involuntary, coercive forms of hierarchy. It radically calls for the abolition of the state which it holds to be undesirable, unnecessary, and harmful.\\n\\nThe history of anarchism goes back to prehistory, when some humans lived "
     ]
    }
   ],
   "source": [
    "! ls /store/dyfar/wikipedia_dump\n",
    "! ls /store/dyfar/wikipedia_dump/AA\n",
    "! head -c 400 /store/dyfar/wikipedia_dump/AA/wiki_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous pouvez le voir, nous avons maintenant un ensemble de dossiers contenant des fichiers qui eux sont composés de 1 article Wikipédia par ligne. Nous montrons aussi les 400 premiers charactères d'un fichier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peuplement de la basse de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import importlib.util\n",
    "import uuid\n",
    "import regex\n",
    "from tqdm.auto  import tqdm\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter('%(asctime)s: [ %(message)s ]', '%m/%d/%Y %I:%M:%S %p')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(fmt)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_files(path):\n",
    "    \"\"\"Walk through all files located under a root path.\"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        yield path\n",
    "    elif os.path.isdir(path):\n",
    "        for dirpath, _, filenames in os.walk(path):\n",
    "            for f in filenames:\n",
    "                yield os.path.join(dirpath, f)\n",
    "    else:\n",
    "        raise RuntimeError('Path %s is invalid' % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction split_document comporte une implémentation naîve et c'est à vous d'implémenter une meilleure fonction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc):\n",
    "    \"\"\"Preprocess a document for exemple if you want to remove stop words\"\"\"\n",
    "    return doc\n",
    "\n",
    "def split_document(doc):\n",
    "    \"\"\"Split a doc on each \".\" character\n",
    "    \n",
    "    Split un document en de plus petites entités et génère un uuid pour \n",
    "    chaque split.\n",
    "    \"\"\"\n",
    "    return [(str(uuid.uuid4()), text) for text in doc.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(filename):\n",
    "    \"\"\"Parse the contents of a file. Each line is a JSON encoded document.\"\"\"\n",
    "    documents = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Parse document\n",
    "            doc = json.loads(line)\n",
    "            # Maybe preprocess the document with custom function\n",
    "            preprocess_doc(doc)\n",
    "            # Skip if it is empty or None\n",
    "            if not doc:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            # Add the document\n",
    "            doc = doc['text']\n",
    "            docs = split_document(doc)\n",
    "            documents += docs\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_contents(data_path, save_path, num_workers=1):\n",
    "    \"\"\"Preprocess and store a corpus of documents in sqlite.\n",
    "    Args:\n",
    "        data_path: Root path to directory (or directory of directories) of files\n",
    "          containing json encoded documents (must have `id` and `text` fields).\n",
    "        save_path: Path to output sqlite db.\n",
    "        preprocess: Path to file defining a custom `preprocess` function. Takes\n",
    "          in and outputs a structured doc.\n",
    "        num_workers: Number of parallel processes to use when reading docs.\n",
    "        \n",
    "    Pour ajouter un document à la BD il faut ajouter une paire <PK, doc> ou PK \n",
    "    est un idenifiant unique et doc le texte du document. Pour générer les PK\n",
    "    nous utilisons un uuid.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(save_path):\n",
    "        raise RuntimeError('%s already exists! Not overwriting.' % save_path)\n",
    "\n",
    "    logger.info('Reading into database...')\n",
    "    conn = sqlite3.connect(save_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"CREATE TABLE documents (id PRIMARY KEY, text);\")\n",
    "\n",
    "    workers = ProcessPool(num_workers)\n",
    "    files = [f for f in iter_files(data_path)]\n",
    "    count = 0\n",
    "    with tqdm(total=len(files)) as pbar:\n",
    "        for pairs in workers.imap_unordered(get_contents, files):\n",
    "            count += len(pairs)\n",
    "            c.executemany(\"INSERT INTO documents VALUES (?,?)\", pairs)\n",
    "            pbar.update()\n",
    "    logger.info('Read %d docs.' % count)\n",
    "    logger.info('Committing...')\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_contents(\"/store/dyfar/wikipedia_dump/\", \"/store/dyfar/doc_test.db\", 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper de la bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def normalize(text):\n",
    "    \"\"\"Resolve different type of unicode encodings.\"\"\"\n",
    "    return unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDB(object):\n",
    "    \"\"\"Sqlite backed document storage.\n",
    "    Implements get_doc_text(doc_id).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_path=None):\n",
    "        self.path = db_path\n",
    "        self.connection = sqlite3.connect(self.path, check_same_thread=False)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def path(self):\n",
    "        \"\"\"Return the path to the file that backs this database.\"\"\"\n",
    "        return self.path\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the connection to the database.\"\"\"\n",
    "        self.connection.close()\n",
    "\n",
    "    def get_doc_ids(self):\n",
    "        \"\"\"Fetch all ids of docs stored in the db.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\"SELECT id FROM documents\")\n",
    "        results = [r[0] for r in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        return results\n",
    "\n",
    "    def get_doc_text(self, doc_id):\n",
    "        \"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT text FROM documents WHERE id = ?\",\n",
    "            (utils.normalize(doc_id),)\n",
    "        )\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        return result if result is None else result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la représentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utiles pour la construction de représentation sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import murmurhash3_32\n",
    "\n",
    "def hash(token, vocab_size):\n",
    "    \"\"\"Unsigned 32 bit murmurhash for feature hashing.\"\"\"\n",
    "    return murmurhash3_32(token, positive=True) % vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche de documents & Ordonnancement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des candidats de réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection de la réponse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
